# Machine Learning Experiments & Data Workflows

A collection of applied machine learning and data exploration projects spanning structured data analysis, text processing, automation, and lightweight API integration.

The repository brings together exploratory analysis, NLP experimentation, web scraping pipelines, and reusable data processing patterns.

---

## Repository Structure

ML/
â”‚
â”œâ”€â”€ API/
â”œâ”€â”€ EDA-Tabular (IRS data set)/
â”œâ”€â”€ EDA-Text/
â”œâ”€â”€ RSS/
â”œâ”€â”€ Scrapy/
â”œâ”€â”€ Web scraping/
â””â”€â”€ .gitignore


---

## Overview

The projects in this repository explore different stages of the data lifecycle:

- Data acquisition from public sources
- Cleaning and preprocessing of structured and unstructured datasets
- Feature extraction and transformation
- Exploratory data analysis
- Lightweight automation workflows
- Integration with external APIs

Each module focuses on a practical use case while maintaining readable and modular code.

---

## Selected Modules

### ğŸ“Š EDA â€“ Tabular Data (IRS Dataset)

Exploration and statistical analysis of structured datasets, including:

- Data profiling and distribution analysis
- Handling missing values
- Feature transformation
- Visualization-driven insight generation

---

### ğŸ“ EDA â€“ Text

Experiments with text normalization and preprocessing:

- Tokenization
- Stopword removal
- Text cleaning strategies
- Basic vectorization techniques

---

### ğŸ•·ï¸ Scrapy & Web Scraping

Automated data collection pipelines using:

- Scrapy spiders
- HTML parsing
- Structured extraction
- Iterative refinement of selectors

---

### ğŸ“° RSS Processing

RSS ingestion and structured parsing workflows for automated data retrieval and transformation.

---

### ğŸ”Œ API Module

Lightweight API interaction and data exchange patterns supporting modular workflows.

---

## Tooling & Environment

- Python
- Pandas / NumPy
- Matplotlib / Seaborn
- Scikit-learn
- BeautifulSoup
- Scrapy
- REST APIs
- Jupyter Notebooks

---

## Notes

- Projects are organized to allow independent experimentation.
- Data preprocessing logic is kept explicit and traceable.
- External credentials are managed through environment variables.
- Directory structure reflects separation between data acquisition and analysis.

---

## Future Directions

- Expanded modeling experiments
- Pipeline orchestration refinements
- Structured evaluation metrics
- Reproducibility improvements
- Extended API integrations
